# GPT-2-fine-tuning

No tenemos suficientes datos como para entrenar desde 0 al algoritmo GPT-2. Por ello, utilizaremos el algoritmo GPT-2 entrenado en inglés (por el momento) para después implementar de alguna manera el poder pasarle archivos .txt nuestros y entrenarlo más a fondo con tantos archivos como le metamos.

Hemos encontrado la siguiente página web que consta de una librería de python la cual facilita mucho el código para entrenar el algoritmo GPT-2 (la versión más simplificada del algoritmo): https://docs.aitextgen.io/tutorials/hello-world/

También hemos encontrado la siguiente página la cual es otra librería para entrenar al algoritmo: https://huggingface.co/docs/transformers/training (el código hecho está hecho con esta librería)

Por último, hemos encontrado la siguiente página la cual consta del algoritmo GPT-2 entrenado en el idioma español, la cual igual utilizaremos en un futuro para poder hacerle preguntas en español: https://huggingface.co/flax-community/gpt-2-spanish/tree/main
