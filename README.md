# GPT-2-fine-tuning

Actualmente hemos entrenado el algoritmo GPT-2 en español y con un archivo .txt, a pesar de esto no se le puede preguntar directamente las cosas, tiene que devolver una continuación de una frase que le pases.

Hemos encontrado la siguiente página web que consta de una librería de python la cual facilita mucho el código para entrenar el algoritmo GPT-2 (la versión más simplificada del algoritmo): https://docs.aitextgen.io/tutorials/hello-world/

También hemos encontrado la siguiente página la cual es otra librería para entrenar al algoritmo: https://huggingface.co/docs/transformers/training (el código hecho está hecho con esta librería)

Por último, hemos encontrado la siguiente página la cual consta del algoritmo GPT-2 entrenado en el idioma español, la cual igual utilizaremos en un futuro para poder hacerle preguntas en español: https://huggingface.co/flax-community/gpt-2-spanish/tree/main
